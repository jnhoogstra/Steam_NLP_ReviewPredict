{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dataset from https://www.kaggle.com/forgemaster/steam-reviews-dataset?select=reviews-1-115.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df1 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df2 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df3 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df4 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df5 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df6 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df7 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df8 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df9 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")\n",
    "df10 = pd.read_csv(\"../../../../trimmed/fps_trimmed_00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d981dd-7090-47c9-9e5e-53f3f33abb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df0.head(3), df1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df0, df1, df2, df3, df4, df5, df6, df7, df8, df9, df10]\n",
    "\n",
    "total_rows = 0\n",
    "for each in df_list:\n",
    "    total_rows += each.shape[0]\n",
    "\n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef11b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we need for NLP is there, but pd.concat still isn't quite hanndling it the way we need\n",
    "# what I can do is fabricate some smaller csvs using train test split (it appears that pandas stops at ~3.06 million rows for this data)\n",
    "# did some math on the side, 19.5% gets us close to our 3.06 million rows\n",
    "\n",
    "# so in order to make a single dataset that's representative of our whole, I need to train test split each df and then combine the test splits into one new_df\n",
    "# and from that hopefully 3 million sized df of random reviews from all of these I can then create train, validation, and test csvs for us to model from in reasonable time\n",
    "\n",
    "# so there's an additional problem, our laptops struggle to train on a dataset larger than 9 thousand reviews\n",
    "# I'm going to use train test split to create dataset of roughly that amount\n",
    "# gonna do that in two splits, and from that create a train, validation, and \n",
    "# .0015\n",
    "rows = 0\n",
    "for ind in range(len(df_list)):\n",
    "    y = df_list[ind][\"voted_up\"] #need to turn this column into 01 format later\n",
    "    X = df_list[ind].drop(\"voted_up\", axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=57, test_size=0.195)\n",
    "    df_list[ind] = pd.concat([X_test, y_test], axis=1)\n",
    "    rows += df_list[ind].shape[0]\n",
    "    \n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c565191-85f1-49ee-98fe-fc1efbd70d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c010f-6292-4e1a-9f84-ada87b3c7da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_list[0]\n",
    "for ind in range(1,11):\n",
    "    new_df = pd.concat([df, df_list[ind]])\n",
    "    df = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b70d71-8bbb-4db5-859a-8af7a7e7a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c879884-704c-4e67-b8b0-81fe1e8c8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so this should be a dataset representative of each of the different CSV\n",
    "# now all I need to do is get a train set of roughly 8k rows, a validation and test set of however big we want\n",
    "# and save those as CSVs for modeling later\n",
    "\n",
    "df.reset_index(drop=True, inplace=True) #run this once\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e6deb-6c11-4cec-86c2-fe5b629a326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb18c0-0069-40c7-a371-b949d0c44da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real quick I'm going to save this as a new csv for potential later use\n",
    "df.to_csv(\"../../../../trimmed/combined_split_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a3c8d-88c7-4067-bf95-39ceaea0b872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
